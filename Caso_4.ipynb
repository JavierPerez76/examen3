{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6faa8f82-8459-4b9d-b8be-b985c9f761af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Limpieza de datos con PySpark: Data Science Job Posting on Glassdoor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffad86de-2091-43f9-97e4-c94456fee761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Los [datos](https://tajamar365.sharepoint.com/:x:/s/3405-MasterIA2024-2025/ETYTQ0c-i6FLjM8rZ4iT1cgB6ipFAkainM-4V9M8DXsBiA?e=PeMtvh) fueron extraídos (scrapeados) del sitio web de Glassdoor y recoge los salarios de distintos puestos relacionados a Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbd64de3-16db-4ce1-89ee-026c842262a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Resolver los siguientes requerimientos, para cada operación/moficación imprima como van quedadndo los cambios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22e55e35-5fe8-4a9a-a14a-d8d15dbe45b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Cargar los datos y mostrar el esquema o la informacion de las columnas y el tip de dato de cada columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51c2c2b7-2c34-4d7f-9672-7cc7ce403aad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- index: string (nullable = true)\n |-- Job Title: string (nullable = true)\n |-- Salary Estimate: string (nullable = true)\n |-- Job Description: string (nullable = true)\n |-- Rating: string (nullable = true)\n |-- Company Name: string (nullable = true)\n |-- Location: string (nullable = true)\n |-- Headquarters: string (nullable = true)\n |-- Size: string (nullable = true)\n |-- Founded: string (nullable = true)\n |-- Type of ownership: string (nullable = true)\n |-- Industry: string (nullable = true)\n |-- Sector: string (nullable = true)\n |-- Revenue: string (nullable = true)\n |-- Competitors: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "               .option(\"delimiter\", \";\") \\\n",
    "               .option(\"multiline\", \"true\") \\\n",
    "               .option(\"quote\", \"\\\"\") \\\n",
    "               .option(\"escape\", \"\\\"\") \\\n",
    "               .csv(\"/FileStore/Examen/Caso4/ds_jobs.csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a4b636b-5e44-4afa-9d44-0666b991cec1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Eliminar duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a95f4d8-2b22-439d-afe1-cc6d7f471b66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# El siguiente comando elimina duplicados. Como no hay duplicados (ya que el index es distinto en todos) no se va a eliminar nada. \n",
    "df = df.dropDuplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9b97746-8b29-4550-9e1b-be2876c2a0a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Decidir que hacer con los datos faltantes \n",
    "\n",
    "Si faltan muchos datos de la empresa como en HireAI, yo eliminaría esas filas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaedda51-4a41-41cd-aba7-75f0e5a3c5f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Decidir que hacer con los valores nulos\n",
    "\n",
    "no hay nulos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbb3b1ca-24ac-4206-8906-3ecee5d6c5aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. ¿Cuántos registros tiene el csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f5ea0af-9814-47dd-a09d-f1b0dfaede12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de registros: 678\n"
     ]
    }
   ],
   "source": [
    "registros = df.count()\n",
    "print(f\"Numero de registros: {registros}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e9aac77-6fe5-4a54-b6ae-c152feee149f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. Mostrar los valores únicos de `Job title` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5d97b28-288e-45a8-a150-ab13d7791581",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+\n|Job Title                                     |\n+----------------------------------------------+\n|Business Intelligence Analyst                 |\n|Data Modeler                                  |\n|Senior Research Statistician- Data Scientist  |\n|Sr Data Scientist                             |\n|Data Scientist/Machine Learning               |\n|Data Scientist / Machine Learning Expert      |\n|Associate Data Scientist                      |\n|Medical Lab Scientist                         |\n|Human Factors Scientist                       |\n|Experienced Data Scientist                    |\n|Data Analyst II                               |\n|Data Scientist                                |\n|Data Analyst                                  |\n|Senior Analyst/Data Scientist                 |\n|Data Scientist-Human Resources                |\n|Data Scientist - Contract                     |\n|Data Scientist - Risk                         |\n|Business Intelligence Analyst I- Data Insights|\n|Staff Data Scientist - Analytics              |\n|Data Scientist - Statistics, Early Career     |\n+----------------------------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "distinct_job = df.select(\"Job Title\").distinct()\n",
    "distinct_job.show(truncate=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df06a700-7dd6-44c8-877c-52819ffb73a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7. Remover la letra `K` de la columna `Salary Estimate` y multiplicar por 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cad8688-678f-460c-933a-84ef687d5fa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "df = df.withColumn(\"Salary Estimate\",  regexp_replace(df[\"Salary Estimate\"], \"K\", \"000\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0f2bbcd-5c78-4d10-9118-e105f611b4af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "8. Mostrar los valores únicos del campo `Salary Estimate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaf2503f-6a22-4e6f-b329-7c89577395b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n|Salary Estimate                 |\n+--------------------------------+\n|$79000-$106000 (Glassdoor est.) |\n|$141000-$225000 (Glassdoor est.)|\n|$112000-$116000 (Glassdoor est.)|\n|$31000-$56000 (Glassdoor est.)  |\n|$56000-$97000 (Glassdoor est.)  |\n|$145000-$225000(Employer est.)  |\n|$122000-$146000 (Glassdoor est.)|\n|$101000-$165000 (Glassdoor est.)|\n|$79000-$133000 (Glassdoor est.) |\n|$99000-$132000 (Glassdoor est.) |\n|$79000-$131000 (Glassdoor est.) |\n|$90000-$109000 (Glassdoor est.) |\n|$69000-$116000 (Glassdoor est.) |\n|$79000-$147000 (Glassdoor est.) |\n|$90000-$124000 (Glassdoor est.) |\n|$91000-$150000 (Glassdoor est.) |\n|$137000-$171000 (Glassdoor est.)|\n|$71000-$123000 (Glassdoor est.) |\n|$110000-$163000 (Glassdoor est.)|\n|$75000-$131000 (Glassdoor est.) |\n+--------------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "distinct_salary = df.select(\"Salary Estimate\").distinct()\n",
    "distinct_salary.show(truncate=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da5d10ab-fd9e-4d8b-bf7f-f8d25b9ed1af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "9. Eliminar `(Glassdoor est.)` y `(Employer est.)` del campo `Salary Estimate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d15c901e-23a0-49a5-814d-1a37b2938488",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, col\n",
    "df = df.withColumn(\"Salary Estimate\", regexp_replace(df[\"Salary Estimate\"], r\"\\(.*?\\)\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80ff37c4-17a0-4eb4-b170-a131bd9b7844",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "10. Mostrar de mayor a menor los valores del campo `Salary Estimate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ac04cda-dd74-4063-a9ff-e8b73b51efe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Salary Estimate\", regexp_replace(df[\"Salary Estimate\"], \"\\\\$\", \"\"))\n",
    "df = df.orderBy(split(col(\"Salary Estimate\"),\"-\")[1].cast(\"int\").desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c20074ca-ff4d-45ba-a548-369bd3bdadc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "11. De la columna `Job Description` quitar los saltos de linea `\\n` del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74ee2a1c-eed8-428a-8890-83ea72a0d7b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Job Description\", regexp_replace(col(\"Job Description\"),\"\\n\",\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63b8b421-3881-4bae-95c0-b79a37422e48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "12. De la columna `Rating` muestre los valores unicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "291748db-d13c-4fe5-9622-0382724c7927",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n|Rating|\n+------+\n|29    |\n|42    |\n|34    |\n|28    |\n|22    |\n|35    |\n|47    |\n|43    |\n|31    |\n|27    |\n|41    |\n|38    |\n|44    |\n|33    |\n|48    |\n|32    |\n|36    |\n|37    |\n|39    |\n|50    |\n+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "distinct_rating = df.select(\"Rating\").distinct()\n",
    "distinct_rating.show(truncate=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd85fcdd-3514-4ad2-ab52-d1f21b825f5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "13. Del campo `Rating` reemplazar los `-1.0` por `0.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6dab52c-2b37-4a12-b041-c9d37fb68241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Rating\", regexp_replace(col(\"Rating\"),\"-1\",\"0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd30cd0d-46a3-4f2f-84f8-4c7e81d7d3ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "14. Mostrar los valores unicos y ordenar los valores del campo `Company Name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5285adbc-6a07-4677-8cdb-fbe95fb76b99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+\n|Company Name                             |\n+-----------------------------------------+\n|Healthfirst\\n3.1                         |\n|iRobot\\n3.5                              |\n|Triplebyte\\n3.2                          |\n|HG Insights\\n4.2                         |\n|Tower Health\\n3.5                        |\n|Guzman & Griffin Technologies (GGTI)\\n4.4|\n|Buckman\\n3.5                             |\n|XSELL Technologies\\n3.6                  |\n|ManTech\\n4.2                             |\n|Intuit - Data\\n4.4                       |\n|Novetta\\n4.5                             |\n|PNNL\\n3.7                                |\n|Analysis Group\\n3.8                      |\n|Affinity Solutions\\n2.9                  |\n|Old World Industries\\n3.1                |\n|1904labs\\n4.7                            |\n|Upside Business Travel\\n4.1              |\n|Novartis\\n3.9                            |\n|INFICON\\n3.5                             |\n|Insight Enterprises, Inc.\\n4.2           |\n+-----------------------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "distinct_company = df.select(\"Company Name\").distinct()\n",
    "distinct_company.show(truncate=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd2e2a45-3570-49a6-823c-882f5714a647",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "15. Quitar todos los caracteres innecesarios que encuentres en el campo `Company Name`. Por ejemplo los saltos de linea `\\n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a1b38ee-4267-4fb9-807d-eabb89abafd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Company Name\", split(col(\"Company Name\"), \"\\n\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e594ec2-2d3f-429a-8cf4-7a6815567aa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "16. En el campo `Location` convertir esa columna en dos: `City` y `State`. Las ciudades que tengas en `Location` asignar a la columna `City`. Lo mismo para `State`. Luego elimine la columna `Location`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a808089-894c-43b3-8593-97697c788447",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"City\", split(col(\"Location\"), \",\")[0])\n",
    "df = df.withColumn(\"State\", regexp_replace(split(col(\"Location\"), \",\")[1], \" \", \"\"))\n",
    "df = df.drop(\"Location\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ed408ad-803a-4209-9dae-57b7418724ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "17. Repetir la misma lógica de la pregunta 16 pero para el campo `Headquarters`. En Headquarters dejar solo la ciudad, mientras que para el estado añadirla a una columna nueva ` Headquarter State`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c246ae3-04b4-4bee-9fa0-6040a190ae3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "headquarters = split(col(\"Headquarters\"), \",\")\n",
    "\n",
    "df = df.withColumn(\"Headquarter State\", headquarters[1]) \n",
    "df = df.withColumn(\"Headquarters\", headquarters[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09c9f37c-dbe5-4f0e-8f9b-7c4b4cfe8ee1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "18. Muestre los valores únicos del campo `Headquarter State` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32c80d4e-ceb1-4011-a9e0-e23a1105442b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n|Headquarter State|\n+-----------------+\n| WA              |\n| MO              |\n| Japan           |\n| NE              |\n| AZ              |\n| TN              |\n| MA              |\n| NY              |\n| OH              |\n| IL              |\n| FL              |\n| LA              |\n| CA              |\n| Switzerland     |\n| Singapore       |\n| PA              |\n| NJ              |\n| GA              |\n| VA              |\n| WI              |\n+-----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "distinct_headquarter = df.select(\"Headquarter State\").distinct()\n",
    "distinct_headquarter.show(truncate=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7895f10-d9c8-443b-a225-848031030eb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "19. Mostrar valores unicos del campo `Size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e75d1eab-93dc-4167-a3d6-25411fbea9b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n|Size                   |\n+-----------------------+\n|-1                     |\n|5001 to 10000 employees|\n|0                      |\n|Unknown                |\n|51 to 200 employees    |\n|1001 to 5000 employees |\n|501 to 1000 employees  |\n|201 to 500 employees   |\n|10000+ employees       |\n|1 to 50 employees      |\n+-----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "distinct_size = df.select(\"Size\").distinct()\n",
    "distinct_size.show(truncate=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8f6a6b8-52b5-4e2e-9850-6dadc97f206c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "20. Quitar 'employee' de los registros del campo `Size`. Elimine tambien otros caracteres basura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4c8c654-7718-4b00-81c4-5f7f30db9234",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Size\", regexp_replace(col(\"Size\"), \"employee\", \"\"))\n",
    "df = df.withColumn(\"Size\", regexp_replace(col(\"Size\"), \"s\", \"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36dfe0a0-9a25-40bc-8282-cba498d2badd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "21. Reemplazar la palabra 'to' por '-' en todos los registros del campo `Size`. Reemplazar tambien '-1' por 'Unknown'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea6b51d4-94c5-41bd-9eeb-79257b1ce29f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Size\", regexp_replace(col(\"Size\"), \" to \", \"-\"))\n",
    "df = df.withColumn(\"Size\", regexp_replace(col(\"Size\"), r\"-1\", \"Unknown\"))\n",
    "df = df.withColumn(\"Size\", regexp_replace(col(\"Size\"), \" \", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f598578e-b86d-4de7-a298-709ec19648ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "22. Mostrar el tipo de dato del campo `Type of ownership` y sus registros unicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "563284bc-383d-4ee8-a015-d97db15326e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- index: string (nullable = true)\n |-- Job Title: string (nullable = true)\n |-- Salary Estimate: string (nullable = true)\n |-- Job Description: string (nullable = true)\n |-- Rating: string (nullable = true)\n |-- Company Name: string (nullable = true)\n |-- Headquarters: string (nullable = true)\n |-- Size: string (nullable = true)\n |-- Founded: string (nullable = true)\n |-- Type of ownership: string (nullable = true)\n |-- Industry: string (nullable = true)\n |-- Sector: string (nullable = true)\n |-- Revenue: string (nullable = true)\n |-- Competitors: string (nullable = true)\n |-- City: string (nullable = true)\n |-- State: string (nullable = true)\n |-- Headquarter State: string (nullable = true)\n\n+------------------------------+\n|Type of ownership             |\n+------------------------------+\n|-1                            |\n|Government                    |\n|Subsidiary or Business Segment|\n|Self-employed                 |\n|Contract                      |\n|Unknown                       |\n|College / University          |\n|Company - Private             |\n|Nonprofit Organization        |\n|Hospital                      |\n|Private Practice / Firm       |\n|Company - Public              |\n|Other Organization            |\n+------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "\n",
    "ownership_distinct = df.select(\"Type of ownership\").distinct()\n",
    "ownership_distinct.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f42ef72-f068-4b46-b1d3-6a5d59290322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "23. Cambiar '-1' por 'Unknown' en todos los registros del campo `Type of ownership`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b05b1d6-afc5-4a7e-b5be-9b77a222dc43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df = df.withColumn(\"Type of ownership\", regexp_replace(\"Type of ownership\", r\"-1\", \"Unknown\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b9f9087-9cce-4213-b99b-f573757dbd97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "24. Cambiar:  \n",
    "-  `Company - Public` por `Public Company`  \n",
    "-  `Company - Private` por `Private Company`  \n",
    "-  `Private Practice / Firm` por `Private Company`  \n",
    "-  `Subsidiary or Business Segment` por `Business`  \n",
    "-  `College / University` por `Education`  \n",
    "En todos los registros del campo `Type of ownership`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4eab34d-9371-4d97-8a55-717eed1d59e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df = df.withColumn(\"Type of ownership\", regexp_replace(\"Type of ownership\", r\"Company - Public\", \"Public Company\"))\n",
    "df = df.withColumn(\"Type of ownership\", regexp_replace(\"Type of ownership\", r\"Company - Private\", \"Private Company\"))\n",
    "df = df.withColumn(\"Type of ownership\", regexp_replace(\"Type of ownership\", r\"Private Practice / Firm\", \"Private Company\"))\n",
    "df = df.withColumn(\"Type of ownership\", regexp_replace(\"Type of ownership\", r\"Subsidiary or Business Segment\", \"Business\"))\n",
    "df = df.withColumn(\"Type of ownership\", regexp_replace(\"Type of ownership\", r\"College / University\", \"Education\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed329cf5-1daa-4dd9-9b16-d2ab23bbf486",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "25. Mostrar el tipo de dato y los valores unicos del campo `Industry`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fd728bc-8db0-4bbd-af70-dabdd6b5cbca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- index: string (nullable = true)\n |-- Job Title: string (nullable = true)\n |-- Salary Estimate: string (nullable = true)\n |-- Job Description: string (nullable = true)\n |-- Rating: string (nullable = true)\n |-- Company Name: string (nullable = true)\n |-- Headquarters: string (nullable = true)\n |-- Size: string (nullable = true)\n |-- Founded: string (nullable = true)\n |-- Type of ownership: string (nullable = true)\n |-- Industry: string (nullable = true)\n |-- Sector: string (nullable = true)\n |-- Revenue: string (nullable = true)\n |-- Competitors: string (nullable = true)\n |-- City: string (nullable = true)\n |-- State: string (nullable = true)\n |-- Headquarter State: string (nullable = true)\n\n+----------------------------------------+\n|Industry                                |\n+----------------------------------------+\n|-1                                      |\n|Investment Banking & Asset Management   |\n|Insurance Carriers                      |\n|Energy                                  |\n|Advertising & Marketing                 |\n|IT Services                             |\n|Chemical Manufacturing                  |\n|Enterprise Software & Network Solutions |\n|Internet                                |\n|Electrical & Electronic Manufacturing   |\n|Express Delivery Services               |\n|Utilities                               |\n|Biotech & Pharmaceuticals               |\n|Staffing & Outsourcing                  |\n|Research & Development                  |\n|Consulting                              |\n|Federal Agencies                        |\n|Computer Hardware & Software            |\n|Consumer Electronics & Appliances Stores|\n|Aerospace & Defense                     |\n+----------------------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.select(\"Industry\").distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c06a12fb-1a33-4cfa-aadb-c01e00a9c4ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "26. En el mismo campo de `Industry` reemplazar '-1' por 'Not Available' y '&' por 'and'.  Vuelva a imprimir los valores unicos en orden alfabético."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2094e39d-0a77-4291-b31f-8a20431a65c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+\n|Industry                                  |\n+------------------------------------------+\n|Accounting                                |\n|Advertising and Marketing                 |\n|Aerospace and Defense                     |\n|Architectural and Engineering Services    |\n|Banks and Credit Unions                   |\n|Biotech and Pharmaceuticals               |\n|Cable, Internet and Telephone Providers   |\n|Chemical Manufacturing                    |\n|Colleges and Universities                 |\n|Computer Hardware and Software            |\n|Construction                              |\n|Consulting                                |\n|Consumer Electronics and Appliances Stores|\n|Consumer Products Manufacturing           |\n|Department, Clothing, and Shoe Stores     |\n|Electrical and Electronic Manufacturing   |\n|Energy                                    |\n|Enterprise Software and Network Solutions |\n|Express Delivery Services                 |\n|Farm Support Services                     |\n+------------------------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"Industry\", regexp_replace(\"Industry\", r\"-1\", \"Not Available\"))\n",
    "df = df.withColumn(\"Industry\", regexp_replace(\"Industry\", r\"&\", \"and\")) \n",
    "\n",
    "df.select(\"Industry\").distinct().orderBy(\"Industry\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2490bb6-1f88-4410-a0c4-75826c48b70c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "27. Para el campo `Sector`, muestre el tipo de dato y los valores únicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f59e0e24-fd1e-44f9-8ea6-f299c22d8c73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- index: string (nullable = true)\n |-- Job Title: string (nullable = true)\n |-- Salary Estimate: string (nullable = true)\n |-- Job Description: string (nullable = true)\n |-- Rating: string (nullable = true)\n |-- Company Name: string (nullable = true)\n |-- Headquarters: string (nullable = true)\n |-- Size: string (nullable = true)\n |-- Founded: string (nullable = true)\n |-- Type of ownership: string (nullable = true)\n |-- Industry: string (nullable = true)\n |-- Sector: string (nullable = true)\n |-- Revenue: string (nullable = true)\n |-- Competitors: string (nullable = true)\n |-- City: string (nullable = true)\n |-- State: string (nullable = true)\n |-- Headquarter State: string (nullable = true)\n\n+----------------------------+\n|Sector                      |\n+----------------------------+\n|Health Care                 |\n|-1                          |\n|Education                   |\n|Insurance                   |\n|Information Technology      |\n|Government                  |\n|Oil, Gas, Energy & Utilities|\n|Finance                     |\n|Real Estate                 |\n|Non-Profit                  |\n|Media                       |\n|Biotech & Pharmaceuticals   |\n|Telecommunications          |\n|Accounting & Legal          |\n|Travel & Tourism            |\n|Aerospace & Defense         |\n|Business Services           |\n|Agriculture & Forestry      |\n|Retail                      |\n|Manufacturing               |\n+----------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.select(\"Sector\").distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0d92b2b-c4be-4aca-b734-3ef8d80b62cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "28. Aplica la misma lógica de la pregunta 26 pero sobre el campo `Sector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "201ce831-fd37-4df7-b2d0-24c98a7d9d72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n|Sector                              |\n+------------------------------------+\n|Accounting and Legal                |\n|Aerospace and Defense               |\n|Agriculture and Forestry            |\n|Biotech and Pharmaceuticals         |\n|Business Services                   |\n|Construction, Repair and Maintenance|\n|Consumer Services                   |\n|Education                           |\n|Finance                             |\n|Government                          |\n|Health Care                         |\n|Information Technology              |\n|Insurance                           |\n|Manufacturing                       |\n|Media                               |\n|Non-Profit                          |\n|Not Available                       |\n|Oil, Gas, Energy and Utilities      |\n|Real Estate                         |\n|Retail                              |\n+------------------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"Sector\", regexp_replace(\"Sector\", r\"-1\", \"Not Available\"))\n",
    "df = df.withColumn(\"Sector\", regexp_replace(\"Sector\", r\"&\", \"and\")) \n",
    "\n",
    "df.select(\"Sector\").distinct().orderBy(\"Sector\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a3770cb-694e-439f-8038-d28da4e3ac0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "29. Para el campo `Revenue`, muestre el tipo de dato y los valores únicos en orden ascedente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6742b510-37ce-498c-8ed8-da0831518467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- index: string (nullable = true)\n |-- Job Title: string (nullable = true)\n |-- Salary Estimate: string (nullable = true)\n |-- Job Description: string (nullable = true)\n |-- Rating: string (nullable = true)\n |-- Company Name: string (nullable = true)\n |-- Headquarters: string (nullable = true)\n |-- Size: string (nullable = true)\n |-- Founded: string (nullable = true)\n |-- Type of ownership: string (nullable = true)\n |-- Industry: string (nullable = true)\n |-- Sector: string (nullable = true)\n |-- Revenue: string (nullable = true)\n |-- Competitors: string (nullable = true)\n |-- City: string (nullable = true)\n |-- State: string (nullable = true)\n |-- Headquarter State: string (nullable = true)\n\n+--------------------------------+\n|Revenue                         |\n+--------------------------------+\n|$1 to $2 billion (USD)          |\n|$1 to $5 million (USD)          |\n|$10 to $25 million (USD)        |\n|$10+ billion (USD)              |\n|$100 to $500 million (USD)      |\n|$2 to $5 billion (USD)          |\n|$25 to $50 million (USD)        |\n|$5 to $10 billion (USD)         |\n|$5 to $10 million (USD)         |\n|$50 to $100 million (USD)       |\n|$500 million to $1 billion (USD)|\n|-1                              |\n|0                               |\n|Less than $1 million (USD)      |\n|Unknown / Non-Applicable        |\n+--------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.select(\"Revenue\").distinct().orderBy(\"Revenue\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cb56eb9-3684-49cd-b20f-fd368d23b499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "30. En el campo `Revenue`, cambiar:  \n",
    "-  `-1` por `N/A`  \n",
    "-  `Unknown / Non-Applicable` por `N/A`  \n",
    "-  `Less than $1 million (USD)` por `Less than 1`\n",
    "-  Quitar `$` y `(USD)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0266a0fb-b6da-423a-a71a-790bea0a96af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n|Revenue                  |\n+-------------------------+\n|0                        |\n|1 to 2 billion           |\n|1 to 5 million           |\n|10 to 25 million         |\n|10+ billion              |\n|100 to 500 million       |\n|2 to 5 billion           |\n|25 to 50 million         |\n|5 to 10 billion          |\n|5 to 10 million          |\n|50 to 100 million        |\n|500 million to 1 billion |\n|Less than 1 million      |\n|N/A                      |\n+-------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"Revenue\", regexp_replace(\"Revenue\", r\"-1\", \"N/A\"))\n",
    "df = df.withColumn(\"Revenue\", regexp_replace(\"Revenue\", r\"Unknown / Non-Applicable\", \"N/A\"))\n",
    "df = df.withColumn(\"Revenue\", regexp_replace(\"Revenue\", r\"Less than $1 million (USD)\", \"Less than 1\"))\n",
    "df = df.withColumn(\"Revenue\", regexp_replace(\"Revenue\", r\"\\$\", \"\")) \n",
    "df = df.withColumn(\"Revenue\", regexp_replace(\"Revenue\", r\"\\(USD\\)\", \"\")) \n",
    "\n",
    "df.select(\"Revenue\").distinct().orderBy(\"Revenue\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d89472f2-855f-4edc-bc47-d2654069bf38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "31. Borrar el campo `Competitors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abe9d863-5d3f-4bbd-bcc9-12f01accac2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(\"Competitors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1d3b8e8-a020-4a19-9d85-fb3f635ed316",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "32. Crear tres columnas: `min_salary` (salario mínimo), `max_salary` (salario maximo) y `avg_salary` (salario promedio) a partir de los datos del campo `Salary Estimate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2186d86b-c908-4dcb-b67a-b1b19a426179",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"min_salary\", split(\"Salary Estimate\", \"-\")[0].cast(\"int\"))\n",
    "df = df.withColumn(\"max_salary\", split(\"Salary Estimate\", \"-\")[1].cast(\"int\"))\n",
    "df = df.withColumn(\"avg_salary\", (col(\"min_salary\") + col(\"max_salary\")) / 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "359b377b-30ee-4c05-b7cd-f0f217a4d9e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "33. Mostrar los valores unicos del campo `Founded` y el tipo de dato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5db6cf5b-aece-4e9e-9cd9-29a4d8514f20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- index: string (nullable = true)\n |-- Job Title: string (nullable = true)\n |-- Salary Estimate: string (nullable = true)\n |-- Job Description: string (nullable = true)\n |-- Rating: string (nullable = true)\n |-- Company Name: string (nullable = true)\n |-- Headquarters: string (nullable = true)\n |-- Size: string (nullable = true)\n |-- Founded: string (nullable = true)\n |-- Type of ownership: string (nullable = true)\n |-- Industry: string (nullable = true)\n |-- Sector: string (nullable = true)\n |-- Revenue: string (nullable = true)\n |-- City: string (nullable = true)\n |-- State: string (nullable = true)\n |-- Headquarter State: string (nullable = true)\n |-- min_salary: integer (nullable = true)\n |-- max_salary: integer (nullable = true)\n |-- avg_salary: double (nullable = true)\n\n+-------+\n|Founded|\n+-------+\n|2016   |\n|2012   |\n|1988   |\n|2017   |\n|2014   |\n|2000   |\n|1965   |\n|1981   |\n|1968   |\n|2011   |\n|1997   |\n|1973   |\n|1996   |\n|1983   |\n|1986   |\n|2015   |\n|1998   |\n|1993   |\n|2010   |\n|1990   |\n+-------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.select(\"Founded\").distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cd2dd8d-9e02-46e5-b441-0b93d2775f05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "34. Reemplazar '-1' por '2024' en todos los registros del campo `Founded`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c8fcd9d-fe21-4925-af06-550dc0bbf0c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Founded\", regexp_replace(\"Founded\", r\"-1\", \"2024\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e823336d-0258-4af5-8d13-c9a358b9295f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "35. Crear una nueva columna o campo que se llame `company_age` con los datos que se deducen del campo `Founded`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b638f50b-459a-4b63-8f96-88d11092dd79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, year\n",
    "\n",
    "current_year = year(current_date())\n",
    "df = df.withColumn(\"company_age\", current_year - df[\"Founded\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49a8f7ed-434c-4ad1-ba31-eeecd65c117c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "36. Crear una columna o campo que se llame: `Job Type` y en cada registro debe ir Senior, Junior o NA según los datos del campo `Job Title`.  \n",
    "- Cambiar 'sr' o 'senior' o 'lead' o 'principal' por `Senior` en el campo `Job Type`. No olvidar las mayúsculas.\n",
    "- Cambiar 'jr' o 'jr.' o cualquier otra variante por `Junior`.  \n",
    "- En cualquier otro caso distinto a los anteriores añadir NA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa802a6c-0fa0-4c2b-9de3-460d346e6dbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df = df.withColumn(\"Job Type\", when(col(\"Job Title\").rlike(\"(?i)sr|senior|lead|principal\"), \"Senior\").when(col(\"Job Title\").rlike(\"(?i)jr|jr\\.\"), \"Junior\").otherwise(\"NA\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c70227b-c16c-4919-9aa2-9bf9514b76dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "37. Muestra los registros únicos del campo `Job Type`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "838048f6-05e3-4ed2-9acf-582748e3c249",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|Job Type|\n+--------+\n|Senior  |\n|NA      |\n|Junior  |\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Job Type\").distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cd4dbad-048d-461f-bf2a-f54c557d16f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "38. Partiendo del campo `Job Description` se extraer todas o las principales skills solicitadas por las empresas, por ejemplo: Python, Spark , Big Data. Cada Skill debe ir en una nueva columna de tipo Binaria ( 0 , 1) o Booleana (True,  False) de modo que cada skill va ser una nueva columna y si esa skill es solicitada por la empresa colocar 1 sino colocar 0. Por ejemplo:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80e86af1-e86e-48db-b9d8-832cee782b01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Por ejemplo:  \n",
    "| Job Title         | Salary Estimate | Job Description                                 | Rating | Company Name       | Size       | Founded | Type of ownership         | Industry                       | Sector                         | Same State      | company_age | Python | Excel |\n",
    "|--------------------|-----------------|-------------------------------------------------|--------|--------------------|------------|---------|---------------------------|--------------------------------|--------------------------------|----------------|-------------|--------|-------|\n",
    "| Sr Data Scientist | 137000-171000   | Description The Senior Data Scientist is resp... | 3.1    | Healthfirst        | 1001-5000  | 1993    | Nonprofit Organization    | Insurance Carriers            | Insurance Carriers            | Same State      | 31          | 0      | 0     |\n",
    "| Data Scientist    | 137000-171000   | Secure our Nation, Ignite your Future Join th... | 4.2    | ManTech            | 5001-10000 | 1968    | Public Company            | Research and Development      | Research and Development      | Same State      | 56          | 0      | 0     |\n",
    "| Data Scientist    | 137000-171000   | Overview Analysis Group is one of the larges... | 3.8    | Analysis Group      | 1001-5000  | 1981    | Private Company           | Consulting                    | Consulting                    | Same State      | 43          | 1      | 1     |\n",
    "| Data Scientist    | 137000-171000   | JOB DESCRIPTION: Do you have a passion for Da... | 3.5    | INFICON            | 501-1000   | 2000    | Public Company            | Electrical and Electronic Manufacturing | Electrical and Electronic Manufacturing | Different State | 24          | 1      | 1     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36fef12f-344b-467c-b349-0cdf5f0a3a53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "skills = [\"Python\", \"Excel\"]\n",
    "\n",
    "for skill in skills:\n",
    "    skill_col = skill.replace(\" \", \"_\")  \n",
    "    df = df.withColumn(\n",
    "        skill_col,\n",
    "        when(\n",
    "            col(\"Job Description\").rlike(f\"(?i)\\\\b{skill}\\\\b\"),\n",
    "            1\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5609a545-1510-4707-a0bd-a937e37675a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "39. Exportar dataset final a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "701378ed-71ce-4908-8a6c-2b4fd6b43763",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2175953469282411>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mheader\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrue\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/FileStore/Examen/Caso4/dataset.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1798\u001B[0m, in \u001B[0;36mDataFrameWriter.csv\u001B[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n",
       "\u001B[1;32m   1779\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode(mode)\n",
       "\u001B[1;32m   1780\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n",
       "\u001B[1;32m   1781\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n",
       "\u001B[1;32m   1782\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1796\u001B[0m     lineSep\u001B[38;5;241m=\u001B[39mlineSep,\n",
       "\u001B[1;32m   1797\u001B[0m )\n",
       "\u001B[0;32m-> 1798\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Path dbfs:/FileStore/Examen/Caso4/dataset.csv already exists."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-2175953469282411>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mheader\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrue\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/FileStore/Examen/Caso4/dataset.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1798\u001B[0m, in \u001B[0;36mDataFrameWriter.csv\u001B[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n\u001B[1;32m   1779\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode(mode)\n\u001B[1;32m   1780\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n\u001B[1;32m   1781\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n\u001B[1;32m   1782\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1796\u001B[0m     lineSep\u001B[38;5;241m=\u001B[39mlineSep,\n\u001B[1;32m   1797\u001B[0m )\n\u001B[0;32m-> 1798\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Path dbfs:/FileStore/Examen/Caso4/dataset.csv already exists.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Path dbfs:/FileStore/Examen/Caso4/dataset.csv already exists.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.write.option(\"header\", \"true\").csv(\"/FileStore/Examen/Caso4/dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41fe24a6-5fc3-4ec4-9b76-56fd2b3e36ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "40. Extraer todos los insights posibles que sean de valor o utilidad. Cree nuevas columnas, agrupar,  filtrar hacer varios plots que muestren dichos insights que sean de utilidad para una empresa o para un usuario. Elabore conclusiones con los insights encontrados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47336ad8-9ded-4bf1-bb31-77bee1ff1d7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n|        Company Name|Rating|\n+--------------------+------+\n|       Aveshka, Inc.|    38|\n|              Aptive|    35|\n|Alaka`ina Foundat...|    36|\n|               Roche|    41|\n|  Maxar Technologies|    35|\n|  Comtech Global Inc|    40|\n|Southwest Researc...|    39|\n|Smith Hanley Asso...|    45|\n|  Maxar Technologies|    35|\n|     Creative Circle|    36|\n|Southwest Researc...|    39|\n|Enterprise Soluti...|    38|\n|              Mackin|    34|\n|   State of Virginia|    32|\n|         Edmunds.com|    34|\n|              Criteo|    39|\n|         AstraZeneca|    40|\n| Oshkosh Corporation|    42|\n|              Leidos|    35|\n|        Cambridge FX|    35|\n+--------------------+------+\nonly showing top 20 rows\n\n+--------------------+------------+--------------+\n|              Sector|sector_count|average_salary|\n+--------------------+------------+--------------+\n|Information Techn...|         188|     118909.57|\n|   Business Services|         122|     129815.57|\n|       Not Available|          71|     129669.01|\n|Biotech and Pharm...|          66|     122871.21|\n|Aerospace and Def...|          47|     133021.28|\n|             Finance|          33|      115500.0|\n|           Insurance|          33|     111651.52|\n|       Manufacturing|          24|     123239.13|\n|         Health Care|          21|      119761.9|\n|          Government|          17|     134470.59|\n|Oil, Gas, Energy ...|          11|      101300.0|\n|              Retail|           7|     150142.86|\n|  Telecommunications|           7|     112357.14|\n|Transportation an...|           6|      117750.0|\n|               Media|           5|      155300.0|\n|  Travel and Tourism|           3|     128666.67|\n|Accounting and Legal|           3|     124666.67|\n|           Education|           3|     117833.33|\n|         Real Estate|           3|     116833.33|\n|Agriculture and F...|           3|     103833.33|\n+--------------------+------------+--------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Averiguar empresas con alta calificacion que pidan python\n",
    "df.filter((col(\"Python\") == 1) & (col(\"Rating\") > 4)).select(\"Company Name\", \"Rating\").show()\n",
    "\n",
    "\n",
    "# Agrupar por el sector, contar las ocurrencias y calcular la media del salario, redondeada a 2 decimales\n",
    "sector_salary_stats = df.groupBy(\"Sector\").agg(\n",
    "    F.count(\"Sector\").alias(\"sector_count\"),  \n",
    "    F.round(F.avg(\"avg_salary\"), 2).alias(\"average_salary\") \n",
    ")\n",
    "sector_salary_stats_sorted = sector_salary_stats.orderBy([\"sector_count\", \"average_salary\"], ascending=[False, False])\n",
    "sector_salary_stats_sorted.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Caso_4",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
